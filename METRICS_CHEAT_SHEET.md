# ðŸ“‹ METRICS CHEAT SHEET - Dragon Fruit Grading System

## Quick Formulas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONFUSION MATRIX: 3Ã—3 grid of Actual vs Predicted grades  â”‚
â”‚                                                             â”‚
â”‚           Predicted                                         â”‚
â”‚        A      B      C                                      â”‚
â”‚    A [TP_A  FP_B  FP_C]  â† Actual samples that are A       â”‚
â”‚    B [FN_A  TP_B  FP_C]  â† Actual samples that are B       â”‚
â”‚    C [FN_A  FN_B  TP_C]  â† Actual samples that are C       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TP = True Positive (diagonal - correct predictions)
FP = False Positive (predicted wrong)
FN = False Negative (missed)
```

---

## 4 Main Metrics

### 1ï¸âƒ£ ACCURACY
```
Formula:  Correct / Total
Example:  22 / 23 = 95.65%
Range:    0% to 100%
Judge by: > 85% is good
```

### 2ï¸âƒ£ PRECISION
```
Formula:  TP / (TP + FP)
Example:  8 / (8+1) = 88.89%
Range:    0% to 100%
Judge by: > 85% means "rarely wrong when I predict A"
```

### 3ï¸âƒ£ RECALL
```
Formula:  TP / (TP + FN)
Example:  8 / (8+1) = 88.89%
Range:    0% to 100%
Judge by: > 85% means "found most of the A's"
```

### 4ï¸âƒ£ F1-SCORE
```
Formula:  2 Ã— (Precision Ã— Recall) / (Precision + Recall)
Example:  2 Ã— (0.89 Ã— 0.89) / (0.89 + 0.89) = 0.89
Range:    0% to 100%
Judge by: > 85% means "good at both precision AND recall"
```

---

## Quick Decision Tree

```
                    Look at Metrics
                           â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚            â”‚            â”‚
        Is Accuracy    Is Precision  Is Recall
         > 85%?         > 85%?        > 85%?
           YES            YES            YES
            â”‚              â”‚              â”‚
            â–¼              â–¼              â–¼
        âœ… Good!      âœ… Not          âœ… Not
                     mislabeling     missing
        
        System works    Be careful     Find all
        overall!        with Grade A   Grade A!
```

---

## Per-Class Analysis

### For each grade (A, B, C), you get:

| Grade | Precision | Recall | F1 | Problem? |
|-------|-----------|--------|----|----|
| A | High âœ… | Low âŒ | Mid âš ï¸ | Missing Grade A |
| A | Low âŒ | High âœ… | Mid âš ï¸ | Mislabeling as Grade A |
| A | High âœ… | High âœ… | High âœ… | No problem! |
| B | Low âŒ | Low âŒ | Low âŒ | Grade B is confused |
| C | High âœ… | High âœ… | High âœ… | No problem! |

---

## What the Numbers Mean

### Grade A is YOUR PREMIUM FRUIT

```
Precision_A = "How often am I RIGHT when I say Premium?"
If 95% â†’ Good! Customers won't get angry
If 60% â†’ Bad! Selling mediocre fruit as premium

Recall_A = "Do I FIND all premium fruit?"
If 95% â†’ Good! Don't lose money on premium
If 60% â†’ Bad! Underselling premium fruit
```

### Grade B is REGULAR FRUIT

```
Precision_B = "How often am I right about regular?"
If 90% â†’ Good! Pricing is accurate
If 70% â†’ Some get wrong grade

Recall_B = "Do I catch all regular fruit?"
If 90% â†’ Good! Consistent grading
If 70% â†’ Some slip into other grades
```

### Grade C is LOW-QUALITY FRUIT

```
Precision_C = "Am I right about low-quality?"
If 95% â†’ Good! Not wasting premium prices on trash
If 60% â†’ Bad! Underselling some fruit

Recall_C = "Do I find all low-quality?"
If 95% â†’ Good! Customers won't complain
If 60% â†’ Bad! Some bad fruit slips through
```

---

## Typical Good Performance

```
ðŸŽ¯ TARGET VALUES:

Accuracy:          > 0.90  (90%+)
Macro F1:          > 0.85  (85%+)
Weighted F1:       > 0.85  (85%+)

Per-class:
  Precision_A:     > 0.90
  Recall_A:        > 0.90
  F1_A:            > 0.90
  (same for B, C)
```

---

## Quick Fixes When Something is Low

### If Accuracy is low (<80%):
```
Problem: Overall system doesn't work
Fix: Check entire pipeline
  - Are thresholds correct?
  - Is fuzzy logic working?
  - Are weights accurate?
```

### If Precision_A is low (<75%):
```
Problem: Marking bad fruit as premium
Fix: Raise threshold for Grade A prediction
  - Make fuzzy logic stricter
  - Require higher score for "Grade A"
```

### If Recall_A is low (<75%):
```
Problem: Missing premium fruit
Fix: Lower threshold for Grade A prediction
  - Make fuzzy logic more lenient
  - Accept medium scores as "Grade A"
```

### If F1 score is low but Accuracy is high:
```
Problem: One or two grades are unbalanced
Fix: Focus on that grade
  - Rebalance fuzzy membership functions
  - Retune rules for problematic grade
```

---

## Example Report Reading

```
YOUR RESULTS:
{
  "accuracy": 0.9565,           âœ… GREAT! 95.65% correct
  "macro_f1": 0.8850,           âœ… GOOD! 88.5% balanced
  "weighted_f1": 0.8947,        âœ… GOOD! Accounts for imbalance
  
  "precision_A": 0.8889,        âœ… Right 88.89% when predicting A
  "recall_A": 0.8889,           âœ… Find 88.89% of real A's
  "f1_A": 0.8889,               âœ… Grade A is balanced
  
  "precision_B": 0.8182,        âš ï¸  Right 81.82% when predicting B
  "recall_B": 0.9000,           âœ… Find 90% of real B's
  "f1_B": 0.8571,               âš ï¸  Grade B needs improvement
  
  "precision_C": 0.8333,        âœ… Right 83.33% when predicting C
  "recall_C": 1.0000,           âœ…âœ… Find 100% of real C's!
  "f1_C": 0.9091,               âœ… Grade C works great!
  
  "confusion_matrix": [
    [8, 1, 0],   â† Grade A: 8 correct, 1 missed, 0 false
    [1, 9, 0],   â† Grade B: 9 correct, some confusion
    [0, 1, 5]    â† Grade C: 5 correct, 1 confusion from B
  ]
}

ANALYSIS:
âœ… Overall system works (95.65% accuracy)
âœ… Grade A is good (premium fruit identified well)
âš ï¸  Grade B has slight precision issue (sometimes marks B when not)
âœ… Grade C is excellent (catches all low-quality)

ACTION: System is ready to deploy!
Minor: Consider tuning Grade B rules if needed
```

---

## Copy-Paste Formulas

```python
# If you need to implement from scratch:

# Accuracy
accuracy = (TP_A + TP_B + TP_C) / Total

# Precision per class
precision_A = TP_A / (TP_A + FP_A)

# Recall per class
recall_A = TP_A / (TP_A + FN_A)

# F1 per class
f1_A = 2 * (precision_A * recall_A) / (precision_A + recall_A)

# Macro average
macro_f1 = (f1_A + f1_B + f1_C) / 3

# Weighted average (if counts are n_A, n_B, n_C)
weighted_f1 = (f1_A * n_A + f1_B * n_B + f1_C * n_C) / Total
```

---

## Remember

| If you see... | It means... | Emoji |
|---|---|---|
| High Accuracy, High F1 | System works! | âœ… |
| High Accuracy, Low F1 | One grade is bad | âš ï¸ |
| Low Accuracy | System broken | âŒ |
| High Precision, Low Recall | Too strict | ðŸš« |
| Low Precision, High Recall | Too lenient | ðŸŽ¯ |
| High Precision, High Recall | Perfect! | ðŸŽ‰ |

---

## Files to Reference

- **METRICS_QUICK_REFERENCE.md** â† You are here
- **METRICS_FORMULAS_GUIDE.md** â† Detailed explanations
- **METRICS_VISUAL_GUIDE.md** â† Diagrams
- **METRICS_MATHEMATICAL_FORMULAS.md** â† Formal math
