{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f33318f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (C:\\Users\\snNHQ\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (C:\\Users\\snNHQ\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Train & Export Notebook â€” Sortir Jagung\n",
    "#\n",
    "# Tujuan: notebook lengkap, dapat dijalankan di Google Colab atau lokal untuk\n",
    "# melatih model klasifikasi (Grade A/B/C) dari folder `dataset/`.\n",
    "# Fitur utama:\n",
    "# - Reproducible (seed)\n",
    "# - Data split (train/val/test 70/20/10)\n",
    "# - Data pipeline tf.data (fast, prefetch)\n",
    "# - Augmentasi modern (Keras preprocessing layers)\n",
    "# - Pilihan model: baseline CNN / MobileNetV2 (transfer learning)\n",
    "# - Callbacks: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
    "# - Evaluation: confusion matrix, classification report\n",
    "# - TFLite conversion (post-training quantization with representative dataset)\n",
    "#\n",
    "# Petunjuk: upload notebook ini ke Google Colab (Runtime: GPU) atau jalankan lokal\n",
    "# setelah menginstal dependensi (tensorflow, scikit-learn, matplotlib, pandas).\n",
    "\n",
    "# %%\n",
    "# Optional: jika di Colab dan ingin install versi terbaru\n",
    "# !pip install -q tensorflow==2.12.0 scikit-learn matplotlib pandas\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TF version', tf.__version__)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Konfigurasi\n",
    "# Ubah path sesuai struktur projek: `dataset/grade_a`, `dataset/grade_b`, `dataset/grade_c`.\n",
    "\n",
    "# %%\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    dataset_dir: str = '../dataset'   # ubah jika perlu\n",
    "    out_dir: str = 'model'\n",
    "    img_size: Tuple[int,int] = (224,224)\n",
    "    batch_size: int = 32\n",
    "    seed: int = 42\n",
    "    epochs: int = 30\n",
    "    val_ratio: float = 0.2\n",
    "    test_ratio: float = 0.1\n",
    "    learning_rate: float = 1e-3\n",
    "    fine_tune_at: int = 100  # layer index for fine-tuning when using transfer learning\n",
    "    pretrained: str = 'mobilenetv2'  # options: baseline, mobilenetv2, efficientnet\n",
    "    class_names: list = None\n",
    "\n",
    "cfg = TrainConfig()\n",
    "Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = str(cfg.seed)\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "tf.random.set_seed(cfg.seed)\n",
    "\n",
    "# Enable mixed precision when GPU available (speeds up on recent GPUs)\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print('Enabled mixed precision policy:', mixed_precision.global_policy())\n",
    "    except Exception as e:\n",
    "        print('Mixed precision not set:', e)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Helper functions\n",
    "\n",
    "# %%\n",
    "def list_image_paths(dataset_dir):\n",
    "    p = Path(dataset_dir)\n",
    "    classes = [d.name for d in sorted(p.iterdir()) if d.is_dir()]\n",
    "    files = []\n",
    "    labels = []\n",
    "    for idx, c in enumerate(classes):\n",
    "        for f in sorted((p/c).glob('*')):\n",
    "            if f.suffix.lower() in ('.jpg','.jpeg','.png','.bmp','.tiff'):\n",
    "                files.append(str(f))\n",
    "                labels.append(idx)\n",
    "    return files, labels, classes\n",
    "\n",
    "\n",
    "def prepare_tf_dataset(file_paths, labels, img_size, batch_size, shuffle=False, augment=False, seed=42):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(file_paths), seed=seed)\n",
    "\n",
    "    def _load(path, label):\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.io.decode_image(image, channels=3, expand_animations=False)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)  # [0,1]\n",
    "        image = tf.image.resize(image, img_size)\n",
    "        return image, tf.one_hot(label, depth=len(cfg.class_names))\n",
    "\n",
    "    ds = ds.map(_load, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if augment:\n",
    "        data_augmentation = keras.Sequential([\n",
    "            layers.RandomFlip('horizontal'),\n",
    "            layers.RandomRotation(0.06),\n",
    "            layers.RandomZoom(0.06),\n",
    "            layers.RandomTranslation(0.05,0.05),\n",
    "        ])\n",
    "        ds = ds.map(lambda x,y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def plot_history(history, out_dir='model'):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend(); plt.grid(True)\n",
    "    plt.title('Loss')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['accuracy'], label='train_acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "    plt.legend(); plt.grid(True)\n",
    "    plt.title('Accuracy')\n",
    "    plt.savefig(Path(cfg.out_dir)/'training_plot.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Load & Split dataset (train / val / test)\n",
    "\n",
    "# %%\n",
    "files, labels, classes = list_image_paths(cfg.dataset_dir)\n",
    "if not files:\n",
    "    raise SystemExit('No images found - pastikan path dataset benar: ' + cfg.dataset_dir)\n",
    "\n",
    "cfg.class_names = classes\n",
    "print('Found classes:', cfg.class_names)\n",
    "\n",
    "# convert to numpy arrays and stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "fp = np.array(files)\n",
    "lab = np.array(labels)\n",
    "\n",
    "# first split off test\n",
    "if cfg.test_ratio > 0:\n",
    "    fp_tmp, fp_test, lab_tmp, lab_test = train_test_split(fp, lab, test_size=cfg.test_ratio, stratify=lab, random_state=cfg.seed)\n",
    "else:\n",
    "    fp_tmp, lab_tmp = fp, lab\n",
    "    fp_test, lab_test = np.array([]), np.array([])\n",
    "\n",
    "# then split train/val\n",
    "val_frac_of_tmp = cfg.val_ratio / (1.0 - cfg.test_ratio)\n",
    "fp_train, fp_val, lab_train, lab_val = train_test_split(fp_tmp, lab_tmp, test_size=val_frac_of_tmp, stratify=lab_tmp, random_state=cfg.seed)\n",
    "\n",
    "print('Counts: train=', len(fp_train), 'val=', len(fp_val), 'test=', len(fp_test))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Build tf.data pipelines\n",
    "\n",
    "# %%\n",
    "train_ds = prepare_tf_dataset(fp_train.tolist(), lab_train.tolist(), cfg.img_size, cfg.batch_size, shuffle=True, augment=True, seed=cfg.seed)\n",
    "val_ds = prepare_tf_dataset(fp_val.tolist(), lab_val.tolist(), cfg.img_size, cfg.batch_size, shuffle=False, augment=False)\n",
    "if len(fp_test):\n",
    "    test_ds = prepare_tf_dataset(fp_test.tolist(), lab_test.tolist(), cfg.img_size, cfg.batch_size, shuffle=False, augment=False)\n",
    "else:\n",
    "    test_ds = None\n",
    "\n",
    "# quick sanity batch\n",
    "for imgs, labs in train_ds.take(1):\n",
    "    print('batch images', imgs.shape, 'labels', labs.shape)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Model definitions\n",
    "# Kita sediakan fungsi untuk: baseline CNN (cepat), MobileNetV2 transfer learning (recommended)\n",
    "\n",
    "# %%\n",
    "\n",
    "def get_baseline(input_shape=(224,224,3), n_classes=3, lr=1e-3):\n",
    "    inp = layers.Input(input_shape)\n",
    "    x = layers.Conv2D(32,3,activation='relu',padding='same')(inp)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Conv2D(64,3,activation='relu',padding='same')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Conv2D(128,3,activation='relu',padding='same')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    out = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_mobilenetv2(input_shape=(224,224,3), n_classes=3, lr=1e-4, fine_tune_at=100):\n",
    "    base = keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base.trainable = True\n",
    "    # freeze until fine_tune_at\n",
    "    for layer in base.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = base(inp, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    out = layers.Dense(n_classes, activation='softmax', dtype='float32')(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Choose & create model\n",
    "\n",
    "# %%\n",
    "if cfg.pretrained == 'baseline':\n",
    "    model = get_baseline(input_shape=(*cfg.img_size,3), n_classes=len(cfg.class_names), lr=cfg.learning_rate)\n",
    "elif cfg.pretrained == 'mobilenetv2':\n",
    "    model = get_mobilenetv2(input_shape=(*cfg.img_size,3), n_classes=len(cfg.class_names), lr=cfg.learning_rate, fine_tune_at=cfg.fine_tune_at)\n",
    "else:\n",
    "    raise ValueError('Unknown model choice')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Callbacks\n",
    "\n",
    "# %%\n",
    "ckpt_path = str(Path(cfg.out_dir)/'best_model.h5')\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(ckpt_path, monitor='val_accuracy', save_best_only=True, save_weights_only=False),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-7),\n",
    "    keras.callbacks.CSVLogger(str(Path(cfg.out_dir)/'training_log.csv')),\n",
    "]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Class weights (optional if imbalance)\n",
    "\n",
    "# %%\n",
    "from collections import Counter\n",
    "cnt = Counter(lab_train)\n",
    "print('Train class counts:', cnt)\n",
    "class_weights = None\n",
    "# if imbalance, compute weights\n",
    "if len(cnt) and max(cnt.values())/min(cnt.values())>1.5:\n",
    "    total = sum(cnt.values())\n",
    "    class_weights = {i: total/(len(cnt)*c) for i,c in cnt.items()}\n",
    "    print('Using class_weights:', class_weights)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Train\n",
    "\n",
    "# %%\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=cfg.epochs, callbacks=callbacks, class_weight=class_weights)\n",
    "\n",
    "# save final model & history\n",
    "model.save(str(Path(cfg.out_dir)/'final_model.h5'))\n",
    "with open(str(Path(cfg.out_dir)/'config.json'),'w') as f:\n",
    "    json.dump(cfg.__dict__, f, indent=2)\n",
    "\n",
    "plot_history(history, out_dir=cfg.out_dir)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Evaluate on test set\n",
    "\n",
    "# %%\n",
    "if test_ds is not None:\n",
    "    best = keras.models.load_model(ckpt_path)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for imgs, labs in test_ds:\n",
    "        preds = best.predict(imgs)\n",
    "        y_pred.extend(np.argmax(preds, axis=1).tolist())\n",
    "        y_true.extend(np.argmax(labs.numpy(), axis=1).tolist())\n",
    "    print('Classification report:')\n",
    "    print(classification_report(y_true, y_pred, target_names=cfg.class_names))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,6));\n",
    "    import seaborn as sns\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=cfg.class_names, yticklabels=cfg.class_names, cmap='Blues')\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')\n",
    "    plt.savefig(Path(cfg.out_dir)/'confusion_matrix.png', dpi=150); plt.show()\n",
    "else:\n",
    "    print('No test dataset (test_ratio==0)')\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Convert to TFLite (with full integer quantization using representative dataset)\n",
    "# This step is optional but recommended for deploying to ESP32/Raspberry Pi.\n",
    "\n",
    "# %%\n",
    "if True:\n",
    "    model_for_export = keras.models.load_model(ckpt_path)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "    # Try float16 optimization when full integer not necessary\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    # Representative dataset for quantization (use a small subset)\n",
    "    def representative_data_gen():\n",
    "        for img, _ in train_ds.unbatch().batch(1).take(100):\n",
    "            # img is float32 in [0,1]\n",
    "            yield [img]\n",
    "\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    # target ops can be set depending on target device\n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "        tflite_path = Path(cfg.out_dir)/'model.tflite'\n",
    "        tflite_path.write_bytes(tflite_model)\n",
    "        print('Saved tflite to', tflite_path)\n",
    "    except Exception as e:\n",
    "        print('TFLite conversion failed:', e)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Next steps / tips\n",
    "# - Jika akurasi belum memuaskan: coba augmentasi lebih kuat, learning rate scheduler, atau transfer learning lain (EfficientNetB0).\n",
    "# - Untuk deployment ke ESP32: gunakan model.tflite dan konversi ke format yang didukung board (ukuran, ops)\n",
    "# - Simpan mapping kelas (`class_names`) di `model/config.json` agar inferensi tahu labelnya.\n",
    "\n",
    "# %%\n",
    "print('Done. Models & artifacts saved to', cfg.out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
